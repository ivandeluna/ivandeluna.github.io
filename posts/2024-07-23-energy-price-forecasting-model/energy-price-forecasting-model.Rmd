---
title: "Energy Price Forecasting Model"
description: "Energy price forecasting model using AutoARIMA and model training for class example."
author: "Iv√°n de Luna-Aldape"
date: "7/23/2024"
categories: 
  - econometrics
  - forecasting
  - machine-learning
  - tutorial
---

```{r setup, inclue = FALSE}
knitr::opts_chunk$set(echo= TRUE)
```
Load required libraries

```{r load_libraries, message = FALSE, warning=FALSE}
library(quantmod)
library(forecast)
library(ggplot2)
library(lubridate)
```

## Download data

In this section, we'll download the historical price data for crude oil futures:

```{r download_data}
download_data <- function(symbol, start_date, end_date){
  getSymbols(symbol, from = start_date, to = end_date, auto.assign = FALSE)
}

# Set parameters
symbol = "CL=F"
start_date = "2020-01-01"
end_date = "2023-12-31"

# Download data
data <- download_data(symbol, start_date, end_date)
```

Downladed data containd an index, which is in date format but must be adapted to time series, also contains prices in open, high, low and close data, volume which informs about the quantity traded and adjusted close prices.

```{r}
cat("Dataset summary:\n")
print(summary(data))
```
We can also check if the time period that was downloaded is the one we specified
```{r}
cat("\nTime period covered:\n")
cat("Start date:", as.Date(index(data)[1]), "\n")
cat("End date:", as.Date(index(data)[nrow(data)]),"\n")
cat("Total number of trading days:", nrow(data),"\n")
```


## Preprocess data
Now, let's process the data by extracting the closing prices and ensuring we have a continous daily time series:

```{r preprocess_data}
preprocess_data <- function(data){
  # Extract closing prices and convert to time series
  close_prices <- as.numeric(data[,4])
  dates <- index(data)
  ts_data <- xts(close_prices, order.by = dates)
  
  # Ensure daily frequency and forward fill missing values
  daily_data <- merge(ts_data, xts(, seq(start(ts_data), 
                                         end(ts_data), 
                                         by="day")))
  daily_data <- na.locf(daily_data)
  
  return(daily_data)
}

processed_data <- preprocess_data(data)
```

We can take a look at our data by ploting the time series:
```{r}
ggplot(data.frame(Date = index(processed_data), 
                  Price = as.numeric(processed_data)), 
       aes(x = Date, y = Price)) +
  geom_line() + 
  labs(title = "Crude oil Futures Closing Prices",
       x = "Date",
       y = "Price (USD)") +
  theme_minimal()
  
```

Calculate and display some statistics
```{r}
cat("\nBasic statistics of closing prices:\n")
print(summary(as.numeric(processed_data)))
```
The graph shows the daily closing prices of crude oil futures over the specified time period. We can observe serevail intersting details:

1. A sharp drop in prices in early 2020, likely due to the COVID-19 pandeminc.
2. A gradual recovery and upward trend from mid-2020 to mid-2022.
3. Some volatility and a slight downward trend in the later part of 2022 and 2023.

These patterns and the overall volatility will be challenging for our forecasting model to capture.

## Split data

We'll now split our data into training and testing sets:

```{r split_data}
split_data <- function(data, test_size = 0.2){
  split_point <- floor(nrow(data) * (1 - test_size))
  train <- data[1:split_point, ]
  test <- data[(split_point + 1):nrow(data), ]
  return(list(train = train, test = test))
}

split <- split_data(processed_data)
train <- split$train
test <- split$test

cat("Training set size:", nrow(train), "days\n")
cat("Test set size:", nrow(test), "days\n")
```

We've split the data so that 80% is used for training and 20% for testing. This allows us to train our model on a 
substantial amount of historical data while still having a significant portion for evaluating its performance.

## Train SARIMA Model

Now, let's train a SARIMA (Seasonal AutoRegressive Integrated Moving Average) model:

```{r train_model}
train_sarima <- function(data){
  # Automatically select the best SARIMA model
  model <- auto.arima(data)
  return(model)
}

model <- train_sarima(train)

summary(model)
```
SARIMA is a popular model for time series forecasting, with the following properties:

1. Seasonal component: it can capture both seasonal and non-seasonal patterns in the data.
2. Autoregressive (AR): It uses past values to predict future values.
3. Integrated (I): It can make the time series stationary by differencing.
4. Moving Average (MA): It uses past forecast erros in the prediction equation.

The `auto.arima()` function automatically selects the best SARIMA model based on the AIC (Akaike Information Criterion). The model summary shows:

- The selected ARIMA order (p,d,q): (0, 1, 1)
- Coefficients in the model: which is an ma1 of -0.3158
- Measures of fit like AIC, BIC, and log-likelihood

This automated approach saves time in model selection, but it's always good to validate the results and potentially try manual parameter tuning if needed.

## Make predictions

Let's use our trained model to make predictions:

```{r make_predictions}
make_predictions <- function(model, n_periods){
  forecast(model,  h = n_periods)
}

predictions <- make_predictions(model, nrow(test))
```

The `forecast()` function generates pint forecasts as well as prediction intervals. The prediction intervals give us an idea of the uncertainty in our forecasts.

```{r predictions_summary}
cat("Predictions mean value\n:")
head(predictions$mean)
cat("Predictions lower values\n:")
head(predictions$lower)
cat("Predictions upper values\n:")
head(predictions$upper)
```


## Evaluate Model

Now, let's evaluate our model's performance:

```{r evaluate_model, warning=FALSE}
evaluate_model <- function(actual, predicted){
  rmse <- sqrt(mean((actual - predicted$mean)^2))
  mae <- mean(abs(actual - predicted$mean))
  mape <- mean(abs((actual - predicted$mean) / actual)) * 100
  return(list(RMSE = rmse, MAE = mae, MAPE = mape))
}

metrics <- evaluate_model(test, predictions)
cat("RMSE:", metrics$RMSE, "\n")
cat("MAE:", metrics$MAE, "\n")
cat("MAPE:", metrics$MAPE, "\n")
```
We are using three common metrics to evaluate our model:

1. Root Mean Square Error (RMSE): Measures the standard deviation of the residuals.
2. Mean Absolute Error (MAE): Measures the average magnitude of the errors in a set of predictions.
3. Mean Absolute Percentage Error (MAPE): Measures accuracy as a percentage, giving us an idea of how far the predictions are off on average.

These metrics give us different perspectives on our model's performance. Lower values indicate better performance.


## Plot results

Finally, let's visualize our results:

```{r plot_results, fig.width=12, fig.height=6}
plot_results <- function(train, test, forecast){
  # Combine data
  all_data <- rbind(train, test)
  
  # Create a data frame for ggplot
  plot_data <- data.frame(
    Date = index(all_data),
    Price = as.numeric(all_data),
    Type = c(rep("Train", nrow(train)), rep("Test", nrow(test)))
  )
  
  forecast_data <- data.frame(
    Date = index(test),
    Price = as.numeric(forecast$mean),
    Type = "Forecast"
  )
  
  plot_data <- rbind(plot_data, forecast_data)
  
  # Create the plot
  ggplot(plot_data, aes(x = Date, y = Price, color = Type)) + 
    geom_line() +
    geom_ribbon(data = forecast_data,
                aes(ymin = forecast$lower[,"95%"],
                    ymax = forecast$upper[,"95%"],
                    fill = Type),
                alpha = 0.2) +
    labs(title = "Energy Price Forecasting",
         x = "Date",
         y = "Price") +
    theme_minimal()
}

plot_results(train, test, predictions)
```

This plot shows:

- The original training data in blue
- The actual test data in green
- Our model's predictions in green
- The shaded area represents the 95% prediction interval for our forecasts
