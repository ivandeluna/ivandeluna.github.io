{"title":"Energy Price Forecasting Model","markdown":{"yaml":{"title":"Energy Price Forecasting Model","description":"Energy price forecasting model using AutoARIMA and model training for class example.","author":"Iván de Luna-Aldape","date":"7/23/2024","categories":["econometrics","forecasting","machine-learning","tutorial"]},"headingText":"Download data","containsRefs":false,"markdown":"\n\n```{r setup, inclue = FALSE}\nknitr::opts_chunk$set(echo= TRUE)\n```\nLoad required libraries\n\n```{r load_libraries, message = FALSE, warning=FALSE}\nlibrary(quantmod)\nlibrary(forecast)\nlibrary(ggplot2)\nlibrary(lubridate)\n```\n\n\nIn this section, we'll download the historical price data for crude oil futures:\n\n```{r download_data}\ndownload_data <- function(symbol, start_date, end_date){\n  getSymbols(symbol, from = start_date, to = end_date, auto.assign = FALSE)\n}\n\n# Set parameters\nsymbol = \"CL=F\"\nstart_date = \"2020-01-01\"\nend_date = \"2023-12-31\"\n\n# Download data\ndata <- download_data(symbol, start_date, end_date)\n```\n\nDownladed data containd an index, which is in date format but must be adapted to time series, also contains prices in open, high, low and close data, volume which informs about the quantity traded and adjusted close prices.\n\n```{r}\ncat(\"Dataset summary:\\n\")\nprint(summary(data))\n```\nWe can also check if the time period that was downloaded is the one we specified\n```{r}\ncat(\"\\nTime period covered:\\n\")\ncat(\"Start date:\", as.Date(index(data)[1]), \"\\n\")\ncat(\"End date:\", as.Date(index(data)[nrow(data)]),\"\\n\")\ncat(\"Total number of trading days:\", nrow(data),\"\\n\")\n```\n\n\n## Preprocess data\nNow, let's process the data by extracting the closing prices and ensuring we have a continous daily time series:\n\n```{r preprocess_data}\npreprocess_data <- function(data){\n  # Extract closing prices and convert to time series\n  close_prices <- as.numeric(data[,4])\n  dates <- index(data)\n  ts_data <- xts(close_prices, order.by = dates)\n  \n  # Ensure daily frequency and forward fill missing values\n  daily_data <- merge(ts_data, xts(, seq(start(ts_data), \n                                         end(ts_data), \n                                         by=\"day\")))\n  daily_data <- na.locf(daily_data)\n  \n  return(daily_data)\n}\n\nprocessed_data <- preprocess_data(data)\n```\n\nWe can take a look at our data by ploting the time series:\n```{r}\nggplot(data.frame(Date = index(processed_data), \n                  Price = as.numeric(processed_data)), \n       aes(x = Date, y = Price)) +\n  geom_line() + \n  labs(title = \"Crude oil Futures Closing Prices\",\n       x = \"Date\",\n       y = \"Price (USD)\") +\n  theme_minimal()\n  \n```\n\nCalculate and display some statistics\n```{r}\ncat(\"\\nBasic statistics of closing prices:\\n\")\nprint(summary(as.numeric(processed_data)))\n```\nThe graph shows the daily closing prices of crude oil futures over the specified time period. We can observe serevail intersting details:\n\n1. A sharp drop in prices in early 2020, likely due to the COVID-19 pandeminc.\n2. A gradual recovery and upward trend from mid-2020 to mid-2022.\n3. Some volatility and a slight downward trend in the later part of 2022 and 2023.\n\nThese patterns and the overall volatility will be challenging for our forecasting model to capture.\n\n## Split data\n\nWe'll now split our data into training and testing sets:\n\n```{r split_data}\nsplit_data <- function(data, test_size = 0.2){\n  split_point <- floor(nrow(data) * (1 - test_size))\n  train <- data[1:split_point, ]\n  test <- data[(split_point + 1):nrow(data), ]\n  return(list(train = train, test = test))\n}\n\nsplit <- split_data(processed_data)\ntrain <- split$train\ntest <- split$test\n\ncat(\"Training set size:\", nrow(train), \"days\\n\")\ncat(\"Test set size:\", nrow(test), \"days\\n\")\n```\n\nWe've split the data so that 80% is used for training and 20% for testing. This allows us to train our model on a \nsubstantial amount of historical data while still having a significant portion for evaluating its performance.\n\n## Train SARIMA Model\n\nNow, let's train a SARIMA (Seasonal AutoRegressive Integrated Moving Average) model:\n\n```{r train_model}\ntrain_sarima <- function(data){\n  # Automatically select the best SARIMA model\n  model <- auto.arima(data)\n  return(model)\n}\n\nmodel <- train_sarima(train)\n\nsummary(model)\n```\nSARIMA is a popular model for time series forecasting, with the following properties:\n\n1. Seasonal component: it can capture both seasonal and non-seasonal patterns in the data.\n2. Autoregressive (AR): It uses past values to predict future values.\n3. Integrated (I): It can make the time series stationary by differencing.\n4. Moving Average (MA): It uses past forecast erros in the prediction equation.\n\nThe `auto.arima()` function automatically selects the best SARIMA model based on the AIC (Akaike Information Criterion). The model summary shows:\n\n- The selected ARIMA order (p,d,q): (0, 1, 1)\n- Coefficients in the model: which is an ma1 of -0.3158\n- Measures of fit like AIC, BIC, and log-likelihood\n\nThis automated approach saves time in model selection, but it's always good to validate the results and potentially try manual parameter tuning if needed.\n\n## Make predictions\n\nLet's use our trained model to make predictions:\n\n```{r make_predictions}\nmake_predictions <- function(model, n_periods){\n  forecast(model,  h = n_periods)\n}\n\npredictions <- make_predictions(model, nrow(test))\n```\n\nThe `forecast()` function generates pint forecasts as well as prediction intervals. The prediction intervals give us an idea of the uncertainty in our forecasts.\n\n```{r predictions_summary}\ncat(\"Predictions mean value\\n:\")\nhead(predictions$mean)\ncat(\"Predictions lower values\\n:\")\nhead(predictions$lower)\ncat(\"Predictions upper values\\n:\")\nhead(predictions$upper)\n```\n\n\n## Evaluate Model\n\nNow, let's evaluate our model's performance:\n\n```{r evaluate_model, warning=FALSE}\nevaluate_model <- function(actual, predicted){\n  rmse <- sqrt(mean((actual - predicted$mean)^2))\n  mae <- mean(abs(actual - predicted$mean))\n  mape <- mean(abs((actual - predicted$mean) / actual)) * 100\n  return(list(RMSE = rmse, MAE = mae, MAPE = mape))\n}\n\nmetrics <- evaluate_model(test, predictions)\ncat(\"RMSE:\", metrics$RMSE, \"\\n\")\ncat(\"MAE:\", metrics$MAE, \"\\n\")\ncat(\"MAPE:\", metrics$MAPE, \"\\n\")\n```\nWe are using three common metrics to evaluate our model:\n\n1. Root Mean Square Error (RMSE): Measures the standard deviation of the residuals.\n2. Mean Absolute Error (MAE): Measures the average magnitude of the errors in a set of predictions.\n3. Mean Absolute Percentage Error (MAPE): Measures accuracy as a percentage, giving us an idea of how far the predictions are off on average.\n\nThese metrics give us different perspectives on our model's performance. Lower values indicate better performance.\n\n\n## Plot results\n\nFinally, let's visualize our results:\n\n```{r plot_results, fig.width=12, fig.height=6}\nplot_results <- function(train, test, forecast){\n  # Combine data\n  all_data <- rbind(train, test)\n  \n  # Create a data frame for ggplot\n  plot_data <- data.frame(\n    Date = index(all_data),\n    Price = as.numeric(all_data),\n    Type = c(rep(\"Train\", nrow(train)), rep(\"Test\", nrow(test)))\n  )\n  \n  forecast_data <- data.frame(\n    Date = index(test),\n    Price = as.numeric(forecast$mean),\n    Type = \"Forecast\"\n  )\n  \n  plot_data <- rbind(plot_data, forecast_data)\n  \n  # Create the plot\n  ggplot(plot_data, aes(x = Date, y = Price, color = Type)) + \n    geom_line() +\n    geom_ribbon(data = forecast_data,\n                aes(ymin = forecast$lower[,\"95%\"],\n                    ymax = forecast$upper[,\"95%\"],\n                    fill = Type),\n                alpha = 0.2) +\n    labs(title = \"Energy Price Forecasting\",\n         x = \"Date\",\n         y = \"Price\") +\n    theme_minimal()\n}\n\nplot_results(train, test, predictions)\n```\n\nThis plot shows:\n\n- The original training data in blue\n- The actual test data in green\n- Our model's predictions in green\n- The shaded area represents the 95% prediction interval for our forecasts\n","srcMarkdownNoYaml":"\n\n```{r setup, inclue = FALSE}\nknitr::opts_chunk$set(echo= TRUE)\n```\nLoad required libraries\n\n```{r load_libraries, message = FALSE, warning=FALSE}\nlibrary(quantmod)\nlibrary(forecast)\nlibrary(ggplot2)\nlibrary(lubridate)\n```\n\n## Download data\n\nIn this section, we'll download the historical price data for crude oil futures:\n\n```{r download_data}\ndownload_data <- function(symbol, start_date, end_date){\n  getSymbols(symbol, from = start_date, to = end_date, auto.assign = FALSE)\n}\n\n# Set parameters\nsymbol = \"CL=F\"\nstart_date = \"2020-01-01\"\nend_date = \"2023-12-31\"\n\n# Download data\ndata <- download_data(symbol, start_date, end_date)\n```\n\nDownladed data containd an index, which is in date format but must be adapted to time series, also contains prices in open, high, low and close data, volume which informs about the quantity traded and adjusted close prices.\n\n```{r}\ncat(\"Dataset summary:\\n\")\nprint(summary(data))\n```\nWe can also check if the time period that was downloaded is the one we specified\n```{r}\ncat(\"\\nTime period covered:\\n\")\ncat(\"Start date:\", as.Date(index(data)[1]), \"\\n\")\ncat(\"End date:\", as.Date(index(data)[nrow(data)]),\"\\n\")\ncat(\"Total number of trading days:\", nrow(data),\"\\n\")\n```\n\n\n## Preprocess data\nNow, let's process the data by extracting the closing prices and ensuring we have a continous daily time series:\n\n```{r preprocess_data}\npreprocess_data <- function(data){\n  # Extract closing prices and convert to time series\n  close_prices <- as.numeric(data[,4])\n  dates <- index(data)\n  ts_data <- xts(close_prices, order.by = dates)\n  \n  # Ensure daily frequency and forward fill missing values\n  daily_data <- merge(ts_data, xts(, seq(start(ts_data), \n                                         end(ts_data), \n                                         by=\"day\")))\n  daily_data <- na.locf(daily_data)\n  \n  return(daily_data)\n}\n\nprocessed_data <- preprocess_data(data)\n```\n\nWe can take a look at our data by ploting the time series:\n```{r}\nggplot(data.frame(Date = index(processed_data), \n                  Price = as.numeric(processed_data)), \n       aes(x = Date, y = Price)) +\n  geom_line() + \n  labs(title = \"Crude oil Futures Closing Prices\",\n       x = \"Date\",\n       y = \"Price (USD)\") +\n  theme_minimal()\n  \n```\n\nCalculate and display some statistics\n```{r}\ncat(\"\\nBasic statistics of closing prices:\\n\")\nprint(summary(as.numeric(processed_data)))\n```\nThe graph shows the daily closing prices of crude oil futures over the specified time period. We can observe serevail intersting details:\n\n1. A sharp drop in prices in early 2020, likely due to the COVID-19 pandeminc.\n2. A gradual recovery and upward trend from mid-2020 to mid-2022.\n3. Some volatility and a slight downward trend in the later part of 2022 and 2023.\n\nThese patterns and the overall volatility will be challenging for our forecasting model to capture.\n\n## Split data\n\nWe'll now split our data into training and testing sets:\n\n```{r split_data}\nsplit_data <- function(data, test_size = 0.2){\n  split_point <- floor(nrow(data) * (1 - test_size))\n  train <- data[1:split_point, ]\n  test <- data[(split_point + 1):nrow(data), ]\n  return(list(train = train, test = test))\n}\n\nsplit <- split_data(processed_data)\ntrain <- split$train\ntest <- split$test\n\ncat(\"Training set size:\", nrow(train), \"days\\n\")\ncat(\"Test set size:\", nrow(test), \"days\\n\")\n```\n\nWe've split the data so that 80% is used for training and 20% for testing. This allows us to train our model on a \nsubstantial amount of historical data while still having a significant portion for evaluating its performance.\n\n## Train SARIMA Model\n\nNow, let's train a SARIMA (Seasonal AutoRegressive Integrated Moving Average) model:\n\n```{r train_model}\ntrain_sarima <- function(data){\n  # Automatically select the best SARIMA model\n  model <- auto.arima(data)\n  return(model)\n}\n\nmodel <- train_sarima(train)\n\nsummary(model)\n```\nSARIMA is a popular model for time series forecasting, with the following properties:\n\n1. Seasonal component: it can capture both seasonal and non-seasonal patterns in the data.\n2. Autoregressive (AR): It uses past values to predict future values.\n3. Integrated (I): It can make the time series stationary by differencing.\n4. Moving Average (MA): It uses past forecast erros in the prediction equation.\n\nThe `auto.arima()` function automatically selects the best SARIMA model based on the AIC (Akaike Information Criterion). The model summary shows:\n\n- The selected ARIMA order (p,d,q): (0, 1, 1)\n- Coefficients in the model: which is an ma1 of -0.3158\n- Measures of fit like AIC, BIC, and log-likelihood\n\nThis automated approach saves time in model selection, but it's always good to validate the results and potentially try manual parameter tuning if needed.\n\n## Make predictions\n\nLet's use our trained model to make predictions:\n\n```{r make_predictions}\nmake_predictions <- function(model, n_periods){\n  forecast(model,  h = n_periods)\n}\n\npredictions <- make_predictions(model, nrow(test))\n```\n\nThe `forecast()` function generates pint forecasts as well as prediction intervals. The prediction intervals give us an idea of the uncertainty in our forecasts.\n\n```{r predictions_summary}\ncat(\"Predictions mean value\\n:\")\nhead(predictions$mean)\ncat(\"Predictions lower values\\n:\")\nhead(predictions$lower)\ncat(\"Predictions upper values\\n:\")\nhead(predictions$upper)\n```\n\n\n## Evaluate Model\n\nNow, let's evaluate our model's performance:\n\n```{r evaluate_model, warning=FALSE}\nevaluate_model <- function(actual, predicted){\n  rmse <- sqrt(mean((actual - predicted$mean)^2))\n  mae <- mean(abs(actual - predicted$mean))\n  mape <- mean(abs((actual - predicted$mean) / actual)) * 100\n  return(list(RMSE = rmse, MAE = mae, MAPE = mape))\n}\n\nmetrics <- evaluate_model(test, predictions)\ncat(\"RMSE:\", metrics$RMSE, \"\\n\")\ncat(\"MAE:\", metrics$MAE, \"\\n\")\ncat(\"MAPE:\", metrics$MAPE, \"\\n\")\n```\nWe are using three common metrics to evaluate our model:\n\n1. Root Mean Square Error (RMSE): Measures the standard deviation of the residuals.\n2. Mean Absolute Error (MAE): Measures the average magnitude of the errors in a set of predictions.\n3. Mean Absolute Percentage Error (MAPE): Measures accuracy as a percentage, giving us an idea of how far the predictions are off on average.\n\nThese metrics give us different perspectives on our model's performance. Lower values indicate better performance.\n\n\n## Plot results\n\nFinally, let's visualize our results:\n\n```{r plot_results, fig.width=12, fig.height=6}\nplot_results <- function(train, test, forecast){\n  # Combine data\n  all_data <- rbind(train, test)\n  \n  # Create a data frame for ggplot\n  plot_data <- data.frame(\n    Date = index(all_data),\n    Price = as.numeric(all_data),\n    Type = c(rep(\"Train\", nrow(train)), rep(\"Test\", nrow(test)))\n  )\n  \n  forecast_data <- data.frame(\n    Date = index(test),\n    Price = as.numeric(forecast$mean),\n    Type = \"Forecast\"\n  )\n  \n  plot_data <- rbind(plot_data, forecast_data)\n  \n  # Create the plot\n  ggplot(plot_data, aes(x = Date, y = Price, color = Type)) + \n    geom_line() +\n    geom_ribbon(data = forecast_data,\n                aes(ymin = forecast$lower[,\"95%\"],\n                    ymax = forecast$upper[,\"95%\"],\n                    fill = Type),\n                alpha = 0.2) +\n    labs(title = \"Energy Price Forecasting\",\n         x = \"Date\",\n         y = \"Price\") +\n    theme_minimal()\n}\n\nplot_results(train, test, predictions)\n```\n\nThis plot shows:\n\n- The original training data in blue\n- The actual test data in green\n- Our model's predictions in green\n- The shaded area represents the 95% prediction interval for our forecasts\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"energy-price-forecasting-model.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","page-footer":{"left":"© 2025 Ivan De Luna. All rights reserved.","right":[{"icon":"github","href":"https://github.com/ivandeluna"}]},"theme":"sandstone","title-block-banner":true,"title":"Energy Price Forecasting Model","description":"Energy price forecasting model using AutoARIMA and model training for class example.","author":"Iván de Luna-Aldape","date":"7/23/2024","categories":["econometrics","forecasting","machine-learning","tutorial"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}